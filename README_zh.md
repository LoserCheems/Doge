# Doge

<h4 align="center">
<p>

[English](./README.md) | 简体中文

</p>
</h4>

## 关于

本项目是对 [Wonderful Matrices](https://arxiv.org/abs/2407.16958) 论文中, 讨论章节部分的延续研究.

我们希望通过训练 `Doge` 架构的小型语言模型(SLM), 来进一步探索 Transformer 框架是否允许更深更复杂的前馈网络结构, 允许模型具有更少的缓存状态与更大的知识容量.

![Doge](./assets/doge_architecture.png)

并且我们希望能够尽可能使用开源工具和框架, 来简化从处理数据到训练模型的流程, 以便于初学者也能够轻易了解和使用.