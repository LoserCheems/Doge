{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doge\n",
    "\n",
    "训练 [Wonderful Matrices](https://arxiv.org/abs/2407.16958) 论文中提出的 `Doge` 小型语言模型.\n",
    "Doge在 Transformers 的框架基础上, 将序列变换部分的 `Multi-Head Attention` 替换为 `Inner Function Attention`, 将状态变换部分的 `MLP` 替换为 `CDMoE` . \n",
    "\n",
    "Train the `Doge` small language model proposed in the paper [Wonderful Matrices](https://arxiv.org/abs/2407.16958).\n",
    "Doge is based on the Transformers framework, replacing the `Multi-Head Attention` in the sequence transformation part with `Inner Function Attention`, and replacing the `MLP` in the state transformation part with `CDMoE`.\n",
    "\n",
    "![doge_architecture](./assets/doge_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载预训练与微调数据集\n",
    "## Download Pre-Training and Fine-Tuning Datasets\n",
    "\n",
    "预训练数据集, 我们选取了 `cosmopedia-v2` 与 `chinese-cosmopedia` 这种高质量合成数据集并补充 `python-edu` 来保证模型的代码能力. \n",
    "\n",
    "For the pre-training dataset, we selected high-quality synthetic datasets such as `cosmopedia-v2` and `chinese-cosmopedia`, and supplemented them with `python-edu` to ensure the model's coding ability.\n",
    "\n",
    "微调数据集, 我们选取了 `Infinity-Instruct` 的 `0625`, `7M` 与 `Gen` 子集.\n",
    "\n",
    "For the fine-tuning dataset, we selected the `0625`, `7M`, and `Gen` subsets of `Infinity-Instruct`.\n",
    "\n",
    "> 请注意: 由于数据集过大, 至少需要 2TB 的存储空间.\n",
    "\n",
    "> Note: Due to the large size of the dataset, at least 2TB of storage space is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填写保存路径, 缓存路径和进程数\n",
    "# Padding save path, cache path and number of processes\n",
    "!python scripts/download_datasets.py --save_dir S:/datasets --cache_dir S:/datasets/cache --num_proc 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理数据集\n",
    "## Preprocess Datasets\n",
    "\n",
    "我们需要使用 `tokenizer` 将数据集转为模型可接受的 `input_ids` 与 `attention_mask`.\n",
    "Doge 使用 `LlamaTokenizer` , 该 tokenizer 词表大小为 `32768` , 使用 `[INST]` 与 `[/INST]` 标记指令. 它还包括工具标记, 但是我们不会在这里使用它们.\n",
    "像 cosmopedia-v2 与 Infinity-Instruct 这样的数据集就包括 `prompt` 与 `text` 两个字段, 我们就将他们标记为用户指令提示与模型输出文本.\n",
    "\n",
    "We need to use the `tokenizer` to convert the dataset into `input_ids` and `attention_mask` that the model can accept.\n",
    "Doge uses the `LlamaTokenizer`, which has a vocabulary size of `32768`, and uses the `[INST]` and `[/INST]` tags to mark instructions. It also includes utility tokens, but we won't use them here.\n",
    "Datasets like cosmopedia-v2 and Infinity-Instruct include two fields, `prompt` and `text`, which we will mark as user instruction prompts and model output text.\n",
    "\n",
    "```python\n",
    "prompt = f\"[INST]{prompt}[/INST]\"\n",
    "return tokenizer(prompt, text, padding='max_length', truncation=True, max_length=MAX_LENGTH)\n",
    "```\n",
    "\n",
    "当然你也可以自行加入一些指令提示.\n",
    "\n",
    "Of course, you can also add some instruction prompts yourself.\n",
    "\n",
    "```python\n",
    "prompt = f\"[INST]You are an AI assistant named `Doge`, you are a language model trained by `Shi Jingze` based on the `Doge` architecture, and your task is to provide appropriate replies and support to users based on their questions and requests.\\n你是一个名为 `Doge` 的人工智能助手, 你是由 `石竞泽` 基于 `Doge` 架构训练的语言模型, 你的任务是针对用户的问题和要求提供适当的答复和支持.\\n[/INST][INST]{prompt}[/INST]\"\n",
    "return tokenizer(prompt, text, padding='max_length', truncation=True, max_length=MAX_LENGTH)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填写数据集路径, 保存路径, 分词器路径, 最大长度和进程数\n",
    "# Padding dataset path, save path, tokenizer path, max length and number of processes\n",
    "!python scripts/preprocess_datasets.py --datasets_dir S:/datasets --save_dir S:/datasets --tokenizer_path ./tokenizer --max_len 2048 --num_proc 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并数据集\n",
    "## Concatenate Datasets\n",
    "\n",
    "我们将 cosmopedia-v2, chinese-cosmopedia 和 python-edu 数据集合并为 `pretrain` 数据集, 将 0625, 7M 和 Gen 数据集合并为 `finetune` 数据集.\n",
    "然后将它们打乱顺序 `seed=233` , 并拆分出来 `1,000` 个样本作为测试集.\n",
    "\n",
    "We combine the cosmopedia-v2, chinese-cosmopedia, and python-edu datasets into the `pretrain` dataset, and the 0625, 7M, and Gen datasets into the `finetune` dataset.\n",
    "Then shuffle the order `seed=233`, and split out `1,000` samples as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填写数据集路径, 保存路径和进程数\n",
    "# Padding dataset path, save path and number of processes\n",
    "!python scripts/merge_datasets.py --datasets_dir S:/datasets --save_dir S:/datasets --num_proc 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置模型参数\n",
    "## Configure Model Parameters\n",
    "\n",
    "我们配置一个 `25M` 的小型模型, 进行训练测试.\n",
    "\n",
    "| Params | n_layers | d_model | n_heads | n_inner_v | d_cross_domain | d_expert | n_exprets | n_expert_heads | n_expert_pre_head |\n",
    "|--------|----------|---------|---------|-----------|----------------|----------|-----------|----------------|-------------------|\n",
    "| 25M    | 8        | 256     | 2       | 2         | 1024           | 256      | 256       | 1              | 2                 |\n",
    "| 80M    | 12       | 512     | 4       | 4         | 2048           | 512      | 512       | 1              | 2                 |\n",
    "| 200M   | 16       | 768     | 6       | 6         | 3072           | 768      | 768       | 2              | 4                 |\n",
    "| 450M   | 24       | 1024    | 8       | 8         | 4096           | 1024     | 1024      | 2              | 4                 |\n",
    "\n",
    "- n_layers 是模型的解码器层数\n",
    "- d_model 是模型的隐藏层维度\n",
    "- n_heads 是InnerFuncAttn的多头注意力头数 d_model // n_heads 最好保持在 64 以上\n",
    "- n_inner_v 是InnerFUncAttn的V的数量 d_model // n_inner_v 最好保持在 64 以上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 32768,\n",
       " 'hidden_size': 256,\n",
       " 'num_hidden_layers': 8,\n",
       " 'hidden_bias': False,\n",
       " 'hidden_dropout': 0.0,\n",
       " 'hidden_act': 'silu',\n",
       " 'max_position_embeddings': 16384,\n",
       " 'rope_theta': 10000.0,\n",
       " 'use_cache': True,\n",
       " 'pad_token_id': 0,\n",
       " 'bos_token_id': 1,\n",
       " 'eos_token_id': 2,\n",
       " 'num_attention_heads': 4,\n",
       " 'num_inner_values': 2,\n",
       " 'cross_domain_intermediate_size': 1024,\n",
       " 'private_expert_intermediate_size': 256,\n",
       " 'num_cdmmoe_experts': 256,\n",
       " 'num_cdmmoe_heads': 1,\n",
       " 'num_cdmmoe_experts_per_head': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from yaml import safe_load\n",
    "\n",
    "# 读取配置文件, 请根据实际情况自行修改\n",
    "# Read the configuration file, please modify it according to the actual situation\n",
    "with open('./model/config/doge_25M.yaml', 'r', encoding='utf-8') as f:\n",
    "    config = safe_load(f)\n",
    "\n",
    "config['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置预训练超参数\n",
    "## Configure Pre-Training Hyperparameters\n",
    "\n",
    "| Params | tokens | num_train_epochs | per_epoch_max_steps | accumulate_steps | learning_rate | warmup_ratio | weight_decay | min_lr_rate |\n",
    "|--------|--------|------------------|---------------------|------------------|---------------|--------------|--------------|-------------|\n",
    "| 25M    | 1B     | 2                | 4,000               | 128              | 8e-4          | 0.1          | 0.01         | 0.1         |\n",
    "| 80M    | 4B     | 2                | 8,000               | 256              | 6e-4          | 0.1          | 0.01         | 0.1         |\n",
    "| 200M   | 16B    | 2                | 16,000              | 512              | 5e-4          | 0.1          | 0.01         | 0.1         |\n",
    "| 450M   | 64B    | 2                | 32,000              | 1024             | 4e-4          | 0.1          | 0.01         | 0.1         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=0,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=400,\n",
      "eval_strategy=steps,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=128,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0008,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./log_doge_25M,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=400,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={'min_lr_rate': 0.1},\n",
      "lr_scheduler_type=cosine_with_min_lr,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./results_doge_25M,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard', 'wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./results_doge_25M,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=400,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.1,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DogeForCausalLM(\n",
      "  (model): DogeModel(\n",
      "    (word_embed): Embedding(32768, 256, padding_idx=0)\n",
      "    (rotary_emb): RotaryEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x DogeDecoderLayer(\n",
      "        (in_attn_layernorm): RMSNorm((256,), eps=1e-06)\n",
      "        (attn): DogeInnerFuncAttn(\n",
      "          (q_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (k_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (v_queries): Linear(in_features=256, out_features=128, bias=False)\n",
      "          (v_embed): Embedding(2, 256)\n",
      "          (o_proj): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (in_ff_layernorm): RMSNorm((256,), eps=1e-06)\n",
      "        (feed_forward): DogeCDMoE(\n",
      "          (act_fn): SiLU()\n",
      "          (shared_up_proj): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (shared_down_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
      "          (queries): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (down_embed): Embedding(256, 256)\n",
      "          (up_embed): Embedding(256, 256)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layernorm): RMSNorm((256,), eps=1e-06)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=32768, bias=False)\n",
      ")\n",
      "24684800\n"
     ]
    }
   ],
   "source": [
    "!python train.py --config_path ./model/config/doge_25M.yaml --logging_dir ./log --output_dir ./results --tokenizer_path ./tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'vocab_size': 32768, 'hidden_size': 1024, 'num_hidden_layers': 16, 'hidden_bias': False, 'hidden_dropout': 0.0, 'hidden_act': 'silu', 'max_position_embeddings': 16384, 'rope_theta': 10000.0, 'use_cache': True, 'pad_token_id': 0, 'bos_token_id': 1, 'eos_token_id': 2, 'num_attention_heads': 8, 'num_inner_values': 8, 'cross_domain_intermediate_size': 4096, 'private_expert_intermediate_size': 1024, 'num_cdmmoe_experts': 1024, 'num_cdmmoe_heads': 2, 'num_cdmmoe_experts_per_head': 4}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DogeForCausalLM(\n",
      "  (model): DogeModel(\n",
      "    (word_embed): Embedding(32768, 1024, padding_idx=0)\n",
      "    (rotary_emb): RotaryEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x DogeDecoderLayer(\n",
      "        (in_attn_layernorm): RMSNorm((1024,), eps=1e-06)\n",
      "        (attn): DogeInnerFuncAttn(\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_queries): Linear(in_features=1024, out_features=128, bias=False)\n",
      "          (v_embed): Embedding(8, 1024)\n",
      "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        )\n",
      "        (in_ff_layernorm): RMSNorm((1024,), eps=1e-06)\n",
      "        (feed_forward): DogeCDMoE(\n",
      "          (act_fn): SiLU()\n",
      "          (shared_up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (shared_down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (queries): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "          (down_embed): Embedding(1024, 1024)\n",
      "          (up_embed): Embedding(1024, 1024)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layernorm): RMSNorm((1024,), eps=1e-06)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=32768, bias=False)\n",
      ")\n",
      "324191232\n"
     ]
    }
   ],
   "source": [
    "!python train.py --config_path ./model/config/doge_320M.yaml --logging_dir ./log --output_dir ./results --tokenizer_path ./tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估\n",
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'lighteval'...\n",
      "Filtering content: 100% (2/2)\n",
      "Filtering content: 100% (2/2), 65.44 MiB | 1.52 MiB/s, done.\n",
      "fatal: active `post-checkout` hook found during `git clone`:\n",
      "\tE:/Doge/lighteval/.git/hooks/post-checkout\n",
      "For security reasons, this is disallowed by default.\n",
      "If this is intentional and the hook should actually be run, please\n",
      "run the command again with `GIT_CLONE_PROTECTION_ACTIVE=false`\n",
      "warning: Clone succeeded, but checkout failed.\n",
      "You can inspect what was checked out with 'git status'\n",
      "and retry with 'git restore --source=HEAD :/'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/lighteval.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'.[accelerate,quantization,adapters]'\"\n"
     ]
    }
   ],
   "source": [
    "!cd lighteval\n",
    "%pip install '.[accelerate,quantization,adapters]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
