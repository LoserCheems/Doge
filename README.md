# Doge

<h4 align="center">
<p>

English | [简体中文](./README_zh.md)

</p>
</h4>

## About

This project is a continuation of the discussion section of the [Wonderful Matrices](https://arxiv.org/abs/2407.16958) paper.

We hope to further explore whether the Transformer framework allows for deeper and more complex feedforward network structures by training a small language model (SLM) based on the `Doge` architecture, allowing the model to have fewer cache states and larger knowledge capacity.

![Doge](./assets/doge_architecture.png)

We also hope to use open-source tools and frameworks as much as possible to simplify the process from data processing to model training, so that beginners can easily understand and use them.